{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from globals import *\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_extension(df):\n",
    "    \"\"\"checks all files have same extension from pandas df, to use in generete sample table function\"\"\"\n",
    "    uniques = df['ext'].unique()\n",
    "    if len(uniques) > 1:\n",
    "        logging.error(f\"{RED}Your input directory has multible fastq file extensions, please check directory.{NC}\")\n",
    "        exit(1)\n",
    "\n",
    "    else:\n",
    "        return uniques\n",
    "\n",
    "def check_PE(df):\n",
    "    \"\"\"checks all files either single or paried ended from pandas df, to use in generete sample table function\"\"\"\n",
    "    uniques = df['PE'].unique()\n",
    "    if len(uniques) > 1:\n",
    "        logging.error(f\"{RED}Your input directory has both Paired and single files, please check directory.{NC}\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        return uniques\n",
    "\n",
    "def check_R(df):\n",
    "    \"\"\"checks all files have same naming patterns from pandas df, to use in generete sample table function\"\"\"\n",
    "    uniques = df['read_num'].unique()\n",
    "    if len(uniques) > 1:\n",
    "        logging.error(f\"{RED}Your input directory has multible fastq file naming patterns, please check directory.{NC}\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        return uniques\n",
    "\n",
    "def check_pattern(df):\n",
    "    \"\"\"checks all files have same naming patterns from pandas df, to use in generete sample table function\"\"\"\n",
    "    uniques = df['matched_pattern'].unique()\n",
    "    if len(uniques) > 1:\n",
    "        logging.error(f\"{RED}Your input directory has multible fastq file naming patterns, please check directory.{NC}\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        return uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recogize_pattern(file_name):\n",
    "    # Matches the file name to a pattern\n",
    "    # naming patterns for sample recognition\n",
    "    patterns = {\n",
    "        \"illumina\": \"(((.+)_(S\\d+)_(L00\\d))_(R1|R2|r1|r2|read1|read2)_(00\\d)\\.(fastq\\.gz|fastq|fq\\.gz|fq))\",\n",
    "        \"SRR\": \"(((SRR)(\\d+))(_|\\.)(1|2|R1|R2|r1|r2|read1|read2)\\.(fastq\\.gz|fastq|fq\\.gz|fq))\",\n",
    "        \"general\": \"(((.+))(_|\\.)(1|2|R1|R2|r1|r2|read1|read2)\\.(fastq\\.gz|fastq|fq\\.gz|fq))\"\n",
    "    }\n",
    "    matched_pattern = None\n",
    "    for ptrn_name, pattern in patterns.items():\n",
    "        try:\n",
    "            matched = re.match(pattern, file_name) \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if bool(matched) :\n",
    "            matched_pattern = ptrn_name\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Extracts sample information\n",
    "\n",
    "    if matched_pattern == \"illumina\":\n",
    "        file_name, sample_name, sample_id, read_num, lane, tail, ext = matched.groups()[0], matched.groups()[1], matched.groups()[2], matched.groups()[5], matched.groups()[4], matched.groups()[6], matched.groups()[7]\n",
    "\n",
    "    elif matched_pattern == \"SRR\":\n",
    "        file_name, sample_name, sample_id, read_num, lane, tail, ext = matched.groups()[0], matched.groups()[1], matched.groups()[3], matched.groups()[5], \"\", \"\", matched.groups()[6]\n",
    "\n",
    "    elif matched_pattern == \"general\":\n",
    "        file_name, sample_name, sample_id, read_num, lane, tail, ext = matched.groups()[0], matched.groups()[1], matched.groups()[1], matched.groups()[4], \"\", \"\", matched.groups()[5]\n",
    "\n",
    "    else:\n",
    "        file_name = sample_name = sample_id = read_num = lane = tail = ext = None\n",
    "\n",
    "    # Returns a dictionary of sample information\n",
    "    return {\n",
    "        \"file_name\": file_name,\n",
    "        \"sample_name\": sample_name,\n",
    "        \"sample_id\": sample_id,\n",
    "        \"read_num\": read_num,\n",
    "        \"lane\": lane,\n",
    "        \"tail\": tail,\n",
    "        \"ext\": ext,\n",
    "        \"matched_pattern\": ptrn_name\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_samples(inpath):\n",
    "    path = os.path.abspath(inpath)\n",
    "    all_files = os.listdir(path)\n",
    "    samples = defaultdict(dict)\n",
    "\n",
    "    for file_name in all_files:\n",
    "        if os.path.isfile(path + \"/\" + file_name) and (\"fastq\" in file_name or \"fq\" in file_name):\n",
    "            # Captures the file path and name\n",
    "            filename, file_extension = os.path.splitext(file_name)\n",
    "            if \"fastq\" in filename or \"fq\" in filename:\n",
    "                filename, new_ext = os.path.splitext(filename)\n",
    "                file_extension = new_ext + file_extension\n",
    "            sample_info = recogize_pattern(file_name)\n",
    "            if \"1\" in sample_info[\"read_num\"]:\n",
    "                read_2 = sample_info[\"read_num\"].replace(\"1\",\"2\")\n",
    "                if sample_info[\"matched_pattern\"] == \"illumina\":\n",
    "                    read_1 = f\"{sample_info['read_num']}_{sample_info['tail']}.f\"\n",
    "                    read_2 = f\"{read_2}_{sample_info['tail']}.f\"\n",
    "                else:\n",
    "                    read_1 = f\"{sample_info['read_num']}.f\"\n",
    "                    read_2 = f\"{read_2}.f\"\n",
    "\n",
    "                f2 = file_name.replace(read_1, read_2)\n",
    "                if f2 in all_files:\n",
    "                    sample_info[\"file2\"] = f2\n",
    "                    sample_info[\"PE\"] = True\n",
    "                    samples[sample_info[\"sample_id\"]] = sample_info\n",
    "\n",
    "                else:\n",
    "                    sample_info[\"file2\"] = \"\"\n",
    "                    sample_info[\"PE\"] = False\n",
    "                    print(f\"{RED}GUAP doesn't support single ended analysis at the moment{NC}\")\n",
    "                    exit(1)\n",
    "\n",
    "    m_samples = samples\n",
    "    samples= pd.DataFrame(samples).T\n",
    "    samples = samples.sort_values(by=['sample_id'])\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = parse_samples(\"/home/abdelrahman/Desktop/data/samples/16s/MiSeq_SOP/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qiime_table_checker(df):\n",
    "    if (\"sample-id\" or \"id\" or \"sampleid\" or \"sample id\") in df.columns:\n",
    "        if \"#q2:types\" in (df.iloc[:1]).values.tolist()[0]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # samples dataframe and output path declaration \n",
    "samples_IDs = list(samples.iloc[:, 2])\n",
    "outpath = os.path.abspath(\"/home/abdelrahman/Desktop/data/samples/16s/\")\n",
    "\n",
    "args = {\n",
    "    \"metadata\": None,\n",
    "    \"create_metadata_table\": False,\n",
    "    \"output\": outpath\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F3D0\n",
      "F3D1\n",
      "F3D141\n",
      "F3D142\n",
      "F3D143\n",
      "F3D144\n",
      "F3D145\n",
      "F3D146\n",
      "F3D147\n",
      "F3D148\n",
      "F3D149\n",
      "F3D150\n",
      "F3D2\n",
      "F3D3\n",
      "F3D5\n",
      "F3D6\n",
      "F3D7\n",
      "F3D8\n",
      "F3D9\n",
      "Mock\n"
     ]
    }
   ],
   "source": [
    "header = pd.DataFrame({\"1\": [\"#q2:types\", \"categorical\"]}).T\n",
    "header.columns = [\"sample-id\", \"condition\"]\n",
    "new_samples = {}\n",
    "c = 2\n",
    "# get sample ids\n",
    "for sample in samples_IDs:\n",
    "    print(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = pd.DataFrame({\"1\": [\"#q2:types\", \"categorical\"]}).T\n",
    "header.columns = [\"sample-id\", \"condition\"]\n",
    "new_samples = {}\n",
    "c = 2\n",
    "# get sample ids\n",
    "for sample in samples_IDs:\n",
    "    new_samples[c] = [sample, \"BLANK\"]\n",
    "    c += 1\n",
    "# store sample IDs and BLANK at new df, and export\n",
    "samples_df = pd.DataFrame(new_samples).T\n",
    "samples_df.columns = [\"sample-id\", \"condition\"]\n",
    "samples_df = samples_df.sort_values([\"sample-id\"])\n",
    "metadata_file = pd.concat([header, samples_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample-id</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#q2:types</td>\n",
       "      <td>categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F3D0</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F3D1</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F3D141</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F3D142</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F3D143</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F3D144</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F3D145</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F3D146</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F3D147</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F3D148</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>F3D149</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>F3D150</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>F3D2</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>F3D3</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>F3D5</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>F3D6</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>F3D7</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>F3D8</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>F3D9</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mock</td>\n",
       "      <td>BLANK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample-id    condition\n",
       "1   #q2:types  categorical\n",
       "2        F3D0        BLANK\n",
       "3        F3D1        BLANK\n",
       "4      F3D141        BLANK\n",
       "5      F3D142        BLANK\n",
       "6      F3D143        BLANK\n",
       "7      F3D144        BLANK\n",
       "8      F3D145        BLANK\n",
       "9      F3D146        BLANK\n",
       "10     F3D147        BLANK\n",
       "11     F3D148        BLANK\n",
       "12     F3D149        BLANK\n",
       "13     F3D150        BLANK\n",
       "14       F3D2        BLANK\n",
       "15       F3D3        BLANK\n",
       "16       F3D5        BLANK\n",
       "17       F3D6        BLANK\n",
       "18       F3D7        BLANK\n",
       "19       F3D8        BLANK\n",
       "20       F3D9        BLANK\n",
       "21       Mock        BLANK"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[;33;1mI will create one at '/home/abdelrahman/Desktop/data/samples/16s', \n",
      "please re-run the analysis with \n",
      "-m /home/abdelrahman/Desktop/data/samples/16s/sample-metada.tsv after modifing the file (check https://docs.qiime2.org/2021.11/tutorials/metadata/)\u001b[;39;m\n",
      "\u001b[;32;1mMetadata empty file created, \u001b[;31;1mExiting...\u001b[;39;0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[;35;1mDuration: 0:11:54.391811\u001b[;39;0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_390915/2427356674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmetadata_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"sample-metadata.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{outpath}/sample-metadata.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mglogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprnt_fatel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{GRE}Metadata empty file created, {RED}Exiting...{NC}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GUAP_toolkit/src/utils/devs/Logger.py\u001b[0m in \u001b[0;36mprnt_fatel\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{str}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{PRP}{runtime.elapsed()}{NC}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# check metadata file \n",
    "if args[\"metadata\"] is None or args[\"create_metadata_table\"]:\n",
    "    if args[\"create_metadata_table\"]:\n",
    "        pass\n",
    "    else:\n",
    "        # prompt the user to create metadata file \n",
    "        create_meta = input(f\"{RED}No metadata file supplied,{NC} do I create an empty one with sample IDs? (y/n) \")\n",
    "        if create_meta == ( 'y' or 'Y'):\n",
    "            pass\n",
    "        else:\n",
    "            # exiting \n",
    "            glogger.prnt_fatel(\"No metadata supplied or created!\")\n",
    "    # creating metadata file \n",
    "    glogger.prnt_warning(f\"{YEL}I will create one at '{outpath}', \\nplease re-run the analysis with \\n-m {outpath}/sample-metada.tsv after modifing the file (check https://docs.qiime2.org/2021.11/tutorials/metadata/)\\033[;39;m\")\n",
    "    # creating empty data frame to store metadata info\n",
    "    header = pd.DataFrame({\"1\": [\"#q2:types\", \"categorical\"]}).T\n",
    "    header.columns = [\"sample-id\", \"condition\"]\n",
    "\n",
    "    new_samples = {}\n",
    "    c = 2\n",
    "    # get sample ids\n",
    "    for sample in samples_IDs:\n",
    "        new_samples[c] = [sample, \"BLANK\"]\n",
    "        c += 1\n",
    "    # store sample IDs and BLANK at new df, and export\n",
    "    samples_df = pd.DataFrame(new_samples).T\n",
    "    samples_df.columns = [\"sample-id\", \"condition\"]\n",
    "    samples_df = samples_df.sort_values([\"sample-id\"])\n",
    "    metadata_file = pd.concat([header, samples_df])\n",
    "    metadata_file.to_csv(outpath+\"/\"+\"sample-metadata.tsv\",sep='\\t',index=False) \n",
    "    args[\"metadata\"] = f\"{outpath}/sample-metadata.tsv\"\n",
    "    glogger.prnt_fatel(f\"{GRE}Metadata empty file created, {RED}Exiting...{NC}\")\n",
    "\n",
    "else:\n",
    "    if not os.path.isfile(args[\"metadata\"]):\n",
    "        glogger.prnt_fatel(f\"{RED}{args['metadata']} Doesn't exist!{NC}\")\n",
    "    else:\n",
    "        # checking metadata file extension \n",
    "        metadataname, metadataextension = os.path.splitext(args[\"metadata\"])\n",
    "        if metadataextension == (\".tsv\"):\n",
    "            the_file = pd.read_csv(args[\"metadata\"],sep=\"\\t\")\n",
    "        elif metadataextension == (\".csv\"):\n",
    "            the_file = pd.read_csv(args[\"metadata\"],sep=\"\\t\")\n",
    "        else:\n",
    "            glogger.prnt_fatel(f\"{RED}{args['metadata']} Have strange extension (use: tsv or csv){NC}\")\n",
    "\n",
    "        rest_of_cols = the_file.columns[1:]\n",
    "        # checks all samples are present in the file with proper names\n",
    "        if False in samples['sample_id'].isin(the_file.T.iloc[0]).tolist():\n",
    "            glogger.prnt_fatel(f\"{RED} Please check {args['metadata']} sample IDs!\\n{YEL}Note: {NC}You can re-run your code WITHOUT suppling a metadata file and \\nGUAP will ask to create an empty one for you with the samples IDs.\")\n",
    "    \n",
    "        else:\n",
    "            # check qiime2 format and modifing if not\n",
    "            if qiime_table_checker(the_file):\n",
    "                if metadataextension == (\".csv\"):\n",
    "                    the_file.to_csv(outpath+\"/\"+\"sample-metadata.tsv\",sep='\\t',index=False) \n",
    "                    args[\"metadata\"] = \"{outpath}/sample-metadata.tsv\"\n",
    "            else:\n",
    "                print(f\"{RED}Note:{NC}Modifing sample metadata file to be compatible with QIIME2\")\n",
    "                columns_names = rest_of_cols.to_list()\n",
    "                columns_names.insert(0, 'sample-id')\n",
    "                columns_names\n",
    "                t = f'{(len(columns_names) -1 )* \"categorical,\"}'.split(\",\")[:-1]\n",
    "                t.insert(0, \"#q2:types\")\n",
    "                header_N = pd.DataFrame({1: t}).T\n",
    "                header_N.columns = columns_names\n",
    "                the_file.columns = columns_names\n",
    "                last_file = pd.concat([header_N, the_file])\n",
    "                last_file.to_csv(outpath+\"/\"+\"sample-metadata.tsv\",sep='\\t',index=False) \n",
    "                args[\"metadata\"] = f\"{outpath}/sample-metadata.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = str(check_pattern(samples)[0])\n",
    "ext = str(check_extension(samples)[0])\n",
    "PE = bool(check_PE(samples))\n",
    "R = str(check_R(samples)[0])\n",
    "compressed = False\n",
    "EXT = ext\n",
    "\n",
    "\n",
    "# to perform gunzipping \n",
    "if \".gz\" in ext:\n",
    "    compressed = True\n",
    "    EXT = ext.replace(\".gz\",\"\")\n",
    "\n",
    "\n",
    "\n",
    "# check if analysis run before and created sample table \n",
    "if os.path.exists(outpath+\"/\"+\"samples.tsv\"):\n",
    "    logging.warning(f\"\\033[;33;1mFound an exsiting sample.tsv file in output directory, will not override.\\033[;39;m\")\n",
    "else:\n",
    "    samples.to_csv(outpath+\"/\"+\"samples.tsv\",sep='\\t')    \n",
    "\n",
    "# create config file \n",
    "with open('config.yaml', 'w') as yaml_file:\n",
    "    yaml.safe_dump(vars(args), yaml_file, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "with open('config.yaml', 'a') as yaml_file:\n",
    "    yaml_file.writelines(f\"path: {path}\\n\")\n",
    "    yaml_file.writelines(f\"working_dir: {outpath}\\n\")\n",
    "    yaml_file.writelines(f\"ext: {ext}\\n\")\n",
    "    yaml_file.writelines(f\"tail: {tail}\\n\")\n",
    "    yaml_file.writelines(f\"R: {R}\\n\")\n",
    "    yaml_file.writelines(f\"R1_pattern: _{R}1{tail}{EXT}\\n\")\n",
    "    yaml_file.writelines(f\"R2_pattern: _{R}2{tail}{EXT}\\n\")\n",
    "    yaml_file.writelines(f\"compressed: {compressed}\\n\")\n",
    "    yaml_file.writelines(f\"total_mem: {all_mem}\\n\")\n",
    "    yaml_file.writelines(f\"GUAP_DIR: {GUAP_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from .globals import *\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # define console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(self.formatter)\n",
    "        self.logger = logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "        # add the file handler to the logger\n",
    "        self.logger.addHandler(console_handler)\n",
    "        # set the logging level based on verbose argument\n",
    "        if verbose:\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "\n",
    "        # define logging methods with color codes\n",
    "        self.prnt_info = lambda str: self.logger.info(f\"{GRY}{str}{NC}\")\n",
    "        self.prnt_warning = lambda str: self.logger.warning(f\"{YEL}{str}{NC}\")\n",
    "        self.prnt_error = lambda str: self.logger.error(f\"{RED}{str}{NC}\")\n",
    "\n",
    "    def add_file_handler(self, file_dir):\n",
    "        # create a file handler\n",
    "        file_handler = logging.FileHandler(file_dir)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        # create a formatter and set it on the file handler\n",
    "        file_handler.setFormatter(self.formatter)\n",
    "\n",
    "        # add the file handler to the logger\n",
    "        self.logger.addHandler(file_handler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
